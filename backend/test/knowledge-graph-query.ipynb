{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    SimpleDirectoryReader, \n",
    "    LLMPredictor,\n",
    "    PromptHelper,\n",
    "    ServiceContext\n",
    ")\n",
    "from llama_index.indices.knowledge_graph.base import GPTKnowledgeGraphIndex\n",
    "import logging\n",
    "import sys\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO) # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, LLMPredictor, ServiceContext\n",
    "from llama_index.indices.knowledge_graph.base import GPTKnowledgeGraphIndex\n",
    "from langchain import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(deployment_name=\"gpt-35-turbo\", model_kwargs={\n",
    "    \"api_key\": openai.api_key,\n",
    "    \"api_base\": openai.api_base,\n",
    "    \"api_type\": openai.api_type,\n",
    "    \"api_version\": openai.api_version,\n",
    "})\n",
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "\n",
    "embedding_llm = LangchainEmbedding(\n",
    "    OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment=\"text-embedding-ada-002\",\n",
    "        openai_api_key= openai.api_key,\n",
    "        openai_api_base=openai.api_base,\n",
    "        openai_api_type=openai.api_type,\n",
    "        openai_api_version=openai.api_version,\n",
    "    ),\n",
    "    embed_batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_titles = [\"Toronto\", \"Seattle\", \"Chicago\", \"Boston\", \"Houston\"]\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "for title in wiki_titles:\n",
    "    response = requests.get(\n",
    "        'https://en.wikipedia.org/w/api.php',\n",
    "        params={\n",
    "            'action': 'query',\n",
    "            'format': 'json',\n",
    "            'titles': title,\n",
    "            'prop': 'extracts',\n",
    "            # 'exintro': True,\n",
    "            'explaintext': True,\n",
    "        }\n",
    "    ).json()\n",
    "    page = next(iter(response['query']['pages'].values()))\n",
    "    wiki_text = page['extract']\n",
    "\n",
    "    data_path = Path('data')\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", 'w', encoding=\"utf-8\") as fp:\n",
    "        fp.write(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_docs = []\n",
    "for wiki_title in wiki_titles:\n",
    "    docs = SimpleDirectoryReader(input_files=[f\"data/{wiki_title}.txt\"]).load_data()\n",
    "    docs[0].doc_id = wiki_title\n",
    "    city_docs.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_predictor_chatgpt = LLMPredictor(llm=llm)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor_chatgpt, chunk_size_limit=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.index_store import MongoIndexStore\n",
    "from llama_index import StorageContext\n",
    "from llama_index.storage.docstore import MongoDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string=\"\"\n",
    "index_store = MongoIndexStore.from_uri(uri=str(connection_string), db_name=\"kg_index\", namespace=\"index\")\n",
    "doc_store = MongoDocumentStore.from_uri(uri=str(connection_string), db_name = \"doc_store\", namespace=\"doc\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "            docstore=doc_store,\n",
    "            index_store=index_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: can take a while! \n",
    "new_index = GPTKnowledgeGraphIndex.from_documents(\n",
    "    docs, \n",
    "    max_triplets_per_chunk=5,\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = new_index.as_query_engine(\n",
    "    include_text=False, \n",
    "    response_mode=\"tree_summarize\"\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"Tell me more about Interleaf\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = new_index.as_query_engine(\n",
    "    include_text=True, \n",
    "    response_mode=\"tree_summarize\"\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"Boston和Chicago有多少人口\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    ResponseSynthesizer\n",
    ")\n",
    "from llama_index.indices.document_summary import GPTDocumentSummaryIndex\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\", model_kwargs={\n",
    "    \"api_key\": openai.api_key,\n",
    "    \"api_base\": openai.api_base,\n",
    "    \"api_type\": openai.api_type,\n",
    "    \"api_version\": openai.api_version,\n",
    "})\n",
    "llm_predictor_chatgpt = LLMPredictor(llm=llm)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=1024, embedding_llm=embedding_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = ResponseSynthesizer.from_args(response_mode=\"tree_summarize\", use_async=True)\n",
    "doc_summary_index = GPTDocumentSummaryIndex.from_documents(\n",
    "    city_docs, \n",
    "    service_context=service_context,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "g = new_index.get_networkx_graph()\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(g)\n",
    "net.show(\"example.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
